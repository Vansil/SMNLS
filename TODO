TODO:
zie ook Google Docs: https://docs.google.com/document/d/1MyKtpJo-vQjtpHbITRMSR5xyMoxn5zrZyDGeWYsIbmc/edit

Other:
- Shared LISA account (lgpu0219, Bjachan69)


PAPER:
[ ] Related Work: herschrijven (moet meer over vorig onderzoek over contextualized word embeddings gaan, o.a. de state-of-the-art (BERT-large) die in de tabel in Results wordt genoemd; minder over de individuele taken, omdat we niet focussen op de performance daarvan; moet ook beter aansluiten op de rest van het paper)
[ ] Methods: BJARNE moet iets schrijven over de loss functie per taak (zie comments in Overleaf)
[ ] Experiments: schrijf subsection over Data
[ ] Voeg p-waarde van MET-NLI:input VS MET-NLI:met toe (TYCHO)
[ ] Eventueel: voeg aan Results een paragraaf toe over de grootste false positives&negatives met een verwijzing naar appendix (alleen als er genoeg ruimte is, of de appendix over de page limit mag gaan -> houd Piazza in de gaten)



WEEK 6
Bjarne:
- [x] Experiment opzetten

Tycho:
- [x] Significance test
- [ ] qualitative error analysis
    - [ ] error analysis function, to identify worst errors
    - [ ] dimensionality reduction
- [~] 5-6 p. paper


Silvan:
- [ ] WiC evaluate for different layers
- [ ] Error analysis help
- [ ] PCA, t-SNE help


WEEK 5

TODO:
Finish and train a model (whatever we'll finish this week will be the main topic of our analysis)
- [x] POS task (dataset available?)
- ~Disambiguation task~
- [x] Sequential labelling task
- [x] Architectures
- [x] Training schemes


Silvan
- ~disamb. train/dev/test split~

Bjarne
- [ ] disamb task
- [x] successive regularization

Tycho









--------------------------------------------
WEEK 4
Silvan:
- Evaluate loaded model
- Mail Katia over POS dataset
- Baseline test: ELMo


Bjarne:
- Dataloader: disambig
- Sequential task





--------------------------------------------
WEEK 3
Bjarne:
- add VUA to get_data.sh
- test Sequential model on VUA
- maak dataloader parallel


Silvan
- general Embedding Module
- Evaluation

Tycho:
- State of the art opzoeken

-------------------------------------------

WEEK2

Evaluation: (TYCHO)
- SentEval
- Word-in-Context
- Extension of TensorBoard metrics, redirecting to the web
    - Gradient norm per layer

Model: 
- Training on Gao's sequential metaphor identification task, combined with NLI (BJARNE)
- Pre-trained ELMo vectors (SILVAN)

Experiment:
- Preprocess data (filter GloVe, ...?)
- Make subsets of data -> overfit
- Train at least one model to ok performance (BJARNE)





